{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting suspicious communication specificly (cyber bulling, threatning, terirosim ) from any type of txt data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO-DO\n",
    "----------\n",
    "Modello:\n",
    "- k-Cross-validation\n",
    "- ridistribuire-dati (10 sospetto 10 non-sospetto)\n",
    "  - questo devo lavorare sui dati dal di fuori con uno script python, cercare di prendere dati 50/50\n",
    "- provare-random-under-sampler\n",
    "- ~~serializzare il modello - pickle~~\n",
    "\n",
    "Estensione:\n",
    "- salvare anche i dati che leggo\n",
    "- in modo da costruire statistiche\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lib\n",
    "----------\n",
    "**NLTK** is a leading platform for building Python programs to work with human language data. The Natural Language Toolkit (NLTK) is an open source Python library for Natural Language Processing.\n",
    "\n",
    "**sklearn** is a free software machine learning library for the Python programming language, it features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN.\n",
    "\n",
    "**Seaborn** is a Python data visualization library based on matplotlib.\n",
    "\n",
    "**pandas** \n",
    "\n",
    "**numpy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re, os\n",
    "for dirname, _, filenames in os.walk('data/dataset'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/dataset/suspicious-tweets.csv')\n",
    "\n",
    "df.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of characters in each message\n",
    "df['length'] = df['message'].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use for pre-test only 11 number of elements!\n",
    "df_labels = df['label']\n",
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for hinstogram\n",
    "count_Class = pd.value_counts(df.label, sort = True)\n",
    "\n",
    "# Data\n",
    "labels = '[0]suspicious', '[1]non-suspicious'\n",
    "sizes = [count_Class[0], count_Class[1]]\n",
    "colors = ['red', 'skyblue']\n",
    "explode = (0.1, 0.1)\n",
    "\n",
    "\n",
    "plt.pie(sizes, explode = explode, labels = labels, colors = colors,\n",
    "        autopct = '%1.1f%%', shadow = True, startangle = 90)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text and clean string\n",
    "def preprocess_text(sen):\n",
    "    sentence = re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)',' ',sen) # Removing html tags\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence) # Remove punctuations and numbers\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence) # Single character removal\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence) # Removing multiple spaces\n",
    "    sentence = sentence.replace(\"ain't\", \"am not\").replace(\"aren't\", \"are not\")\n",
    "    sentence = ' '.join(text.lower() for text in sentence.split(' ')) # Lowering cases\n",
    "    sw = stopwords.words('english')\n",
    "    sentence = ' '.join(text for text in sentence.split() if text not in sw) #removing stopwords\n",
    "    #sentence = ' '.join(text.lemmatize() for text in sentence.split()) #lemmatization\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lib\n",
    "--------------\n",
    "The **Porter stemming** algorithm (or 'Porter stemmer') is a process for removing the commoner morphological and inflexional endings from words in English. Its main use is as part of a term normalisation process that is usually done when setting up Information Retrieval systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in message, clean text\n",
    "df['message'] = df.message.apply(preprocess_text)\n",
    "# Tokenization each text\n",
    "df['message'] = df['message'].apply(nltk.word_tokenize)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "df['message'] = df['message'].apply(lambda x: [stemmer.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\n",
    "# -> ['upset', 'updat', 'facebook', 'text', 'might', 'cri', 'result', 'school', 'today', 'blah']\n",
    "show = df['message']\n",
    "show.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lib\n",
    "-------------------\n",
    "**CountVectorizer**\n",
    "Convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "**TfidfTransformer**\n",
    "Transform a count matrix to a normalized tf or tf-idf representation.\n",
    "\n",
    "Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval, that has also found good use in document classification.\n",
    "\n",
    "The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message'] = df['message'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Convert a collection of text documents to a matrix of token counts.\n",
    "count_vect = CountVectorizer()\n",
    "counts = count_vect.fit_transform(df['message']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer().fit(counts)\n",
    "counts = transformer.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separe dataset in traning and test(30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts, df['label'], test_size=0.3, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing naive bayes\n",
    "NB = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "predicted = NB.predict(X_test)\n",
    "print(np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Confronto etichette corrette dei dati di test (y) con le risposte elaborate dal programma (predicted).\n",
    "error = accuracy_score(y_test, predicted)\n",
    "print(f\"Accuracy: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test,predicted)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix to evaluate the accuracy of a classification\n",
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "----------\n",
    "Le macchine vettoriali di supporto (SVM) sono un insieme di metodi di apprendimento supervisionato utilizzati per la classificazione, la regressione e il rilevamento di anomalie.\n",
    "\n",
    "I vantaggi delle macchine a vettori di supporto sono\n",
    "- Efficaci in spazi ad alta dimensionalità.\n",
    "- Sono ancora efficaci nei casi in cui il numero di dimensioni è maggiore del numero di campioni.\n",
    "- Utilizza un sottoinsieme di punti di addestramento nella funzione di decisione (chiamati vettori di supporto), quindi è anche efficiente dal punto di vista della memoria.\n",
    "- Versatile: è possibile specificare diverse funzioni kernel per la funzione di decisione. Vengono forniti kernel comuni, ma è anche possibile specificare kernel personalizzati.\n",
    "\n",
    "Gli svantaggi delle macchine vettoriali di supporto sono i seguenti:\n",
    "- Se il numero di caratteristiche è molto maggiore del numero di campioni, è fondamentale evitare l'over-fitting nella scelta delle funzioni Kernel e del termine di regolarizzazione.\n",
    "- Le SVM non forniscono direttamente stime di probabilità, che vengono calcolate utilizzando una costosa convalida incrociata a cinque volte (vedere Punteggi e probabilità, più avanti).\n",
    "Le macchine vettoriali di supporto in scikit-learn supportano come input vettori campione sia densi (numpy.ndarray e convertibili in numpy.asarray) che radi (qualsiasi scipy.sparse). Tuttavia, per utilizzare una SVM per fare previsioni su dati sparsi, è necessario che sia stata adattata a tali dati. Per ottenere prestazioni ottimali, utilizzare numpy.ndarray ordinati in C (densi) o scipy.sparse.csr_matrix (radi) con dtype=float64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing SVM\n",
    "sv = SVC().fit(X_train, y_train)\n",
    "\n",
    "predicted = sv.predict(X_test)\n",
    "print(np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "------------\n",
    "Compute confusion matrix to evaluate the accuracy of a classification.\n",
    "\n",
    "By definition a confusion matrix `C` is such that `C_{i, j}` is equal to the number of observations known to be in group `i` and predicted to be in group `j`.\n",
    "\n",
    "Thus in binary classification, the count of true negatives is `C_{0,0}`, false negatives is `C_{1,0}`, true positives is `C_{1,1}` and false positives is `C_{0,1}`.\n",
    "\n",
    "##### Return:\n",
    "`C `: ndarray of shape (n_classes, n_classes)\n",
    "    Confusion matrix whose `i-th` row and `j-th` column entry indicates the number of samples with true label being `i-th` class and predicted label being `j-th` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree classifier\n",
    "------------\n",
    "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing disession tree\n",
    "dt = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "predicted = dt.predict(X_test)\n",
    "print(np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An AdaBoost classifier\n",
    "------------\n",
    "An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\n",
    "\n",
    "This class implements the algorithm known as AdaBoost-SAMME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing adabost\n",
    "ab = AdaBoostClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = ab.predict(X_test)\n",
    "\n",
    "print(np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lib\n",
    "-------------\n",
    "**Transform** documents to document-term matrix.\n",
    "Extract token counts out of raw text documents using the vocabulary fitted with fit or the one provided to the constructor.\n",
    "\n",
    "**Predict** class or regression value for X.\n",
    "For a classification model, the predicted class for each sample in X is returned. For a regression model, the predicted value based on X is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting content to lower case\n",
    "pred = (df['message'].str.lower())\n",
    "# printing predictions made by model\n",
    "print(\"prediction: {}\". format(dt.predict(count_vect.transform(pred.values.astype('U')))))\n",
    "# saving predictions in a variable\n",
    "my_pred = dt.predict(count_vect.transform(pred.values.astype('U')))\n",
    "\n",
    "# saving predicted labels in .csv file\n",
    "df['autotag'] = my_pred\n",
    "df.to_csv('data/dataset/result.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the number of -ve , +ve  predictions \n",
    "positive = 0\n",
    "negative = 0\n",
    "\n",
    "for v in my_pred:\n",
    " if (v == 1):\n",
    "  positive += 1\n",
    " elif (v == 0):\n",
    "  negative += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for quality measurement (can be set to required parameters)\n",
    "def quality(pos,total):\n",
    "    if((pos*100)/total >= 0 and (pos*100)/total <=24 ):\n",
    "        print('Quality: Very Negative')\n",
    "    elif((pos*100)/total >= 25 and (pos*100)/total <=49 ):\n",
    "        print('Quality: Negative')\n",
    "    elif((pos*100)/total >= 50 and (pos*100)/total <=74 ):\n",
    "        print('Quality: Positive')\n",
    "    elif((pos*100)/total >= 75 and (pos*100)/total <=100 ):\n",
    "        print('Quality: Very Positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing library to plot charts\n",
    "import matplotlib.pyplot as plt \n",
    "  \n",
    "# x-coordinates of left sides of bars  \n",
    "left = [100, 200 ] \n",
    "  \n",
    "# heights of bars \n",
    "height = [positive,negative] \n",
    "  \n",
    "# labels for bars \n",
    "tick_label = ['Positive', 'Negative'] \n",
    "  \n",
    "# plotting a bar chart \n",
    "plt.bar(left, height, tick_label = tick_label, width = 80, color = ['green','blue']) \n",
    "  \n",
    "# naming the x-axis \n",
    "plt.xlabel('Sentiment') \n",
    "# naming the y-axis \n",
    "plt.ylabel('Reviews') \n",
    "# plot title \n",
    "plt.title('Sentiment Analysis') \n",
    "  \n",
    "# function to show the plot \n",
    "plt.show() \n",
    "print (\"Sentiment - Positive: \"+str(positive)+\", Negative: \"+str(negative))\n",
    "quality(positive,(positive+negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting negativity \n",
    "negativeity = (negative*100)/(positive+negative)\n",
    "positivity = (positive*100)/(positive+negative)\n",
    "print(\"Negativeity: \"+str(negativeity)+\"%\")\n",
    "print(\"Positivity: \"+str(positivity)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(dt, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5030792b3492f6b12d94f1f48beca3d8e59ec05fd59d0aaaa48e684281ed297"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
